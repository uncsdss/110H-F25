{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30279bd3-481b-4ca7-a20b-350d8334ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbc6f3-6222-40ba-9895-08f5be51afad",
   "metadata": {},
   "source": [
    "# Underfitting and overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce24d2",
   "metadata": {},
   "source": [
    "## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545282a-bb82-4e09-b47e-26fa3212593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "titanic = titanic.drop(columns=['Name']) # drop the column 'Name'\n",
    "is_F = (titanic['Sex']=='female') # array of True and False\n",
    "titanic['Sex'] = is_F.astype(int) # 1 = female, 0 = male\n",
    "train = titanic.sample(frac=0.8) # 80% rows for training\n",
    "test = titanic.drop(index=train.index)\n",
    "\n",
    "y_train = train['Survived']\n",
    "X_train = train.drop(columns=['Survived'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Survived']\n",
    "X_test = test.drop(columns=['Survived']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56931b2",
   "metadata": {},
   "source": [
    "### Let's fit two trees: one with `max_depth=2` and the other with `max_depth=20`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd0f41-6d58-403d-b504-4d8a2961970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "T2 = tree.DecisionTreeClassifier(max_depth=2)\n",
    "T20 = tree.DecisionTreeClassifier(max_depth=20)\n",
    "\n",
    "T2.fit(X_train, y_train)\n",
    "T20.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30552a-b474-47d3-90e1-d2a351fed4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "p = tree.plot_tree(T2, \n",
    "                   filled=True, \n",
    "                   feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b26e7-df3a-4d7d-84cc-13a1bc71b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "p = tree.plot_tree(T20, \n",
    "                   filled=True, \n",
    "                   feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc3bf5",
   "metadata": {},
   "source": [
    "### Evaluate depth 2 tree and depth 20 tree\n",
    "\n",
    "We see that the training accuracy of the depth-2 tree is lower than that of the depth-20 tree, while the test accuracy of the depth-2 tree is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac859ee-aa11-4beb-bb17-df49e5884840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max_depth 2')\n",
    "print('Train score:',T2.score(X_train, y_train))\n",
    "print('Test score:',T2.score(X_test, y_test))\n",
    "print()\n",
    "print('max_depth 20')\n",
    "print('Train score:',T20.score(X_train, y_train))\n",
    "print('Test score:',T20.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c316b",
   "metadata": {},
   "source": [
    "### Investigate how the tree's `max_depth` affects training and test performance by varying the depth from 1 to 30.\n",
    "\n",
    "As the depth of the tree increases from 1 to 30, training accuracy (in blue) consistently improves and eventually reaches 100%, indicating that deeper trees can perfectly fit the training data. In contrast, test accuracy (in orange) initially increases slightly but then fluctuates and generally declines as depth increases. This figure illustrates overfitting: deeper trees capture noise in the training data, resulting in reduced generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb67e30-708b-4f65-9e33-eb536c833402",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    T = tree.DecisionTreeClassifier(max_depth=depth, criterion='gini')\n",
    "    T.fit(X_train, y_train)\n",
    "    train_scores.append(T.score(X_train, y_train))\n",
    "    test_scores.append(T.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x=depths, y=train_scores, label='train')\n",
    "sns.scatterplot(x=depths, y=test_scores, label='test')\n",
    "ax.set_xlabel('Depth of tree')\n",
    "ax.set_ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb593a1-01bb-42f2-8c27-9e5cc9e7d463",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54895b9",
   "metadata": {},
   "source": [
    "Let's simulate the data points for this example. We assume the following linear relationship: $Y=X+1 + 0.2\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10360291-9df6-4303-a5c3-8bdf9b7adaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls random number generation\n",
    "# always get the same data\n",
    "np.random.seed(1234) \n",
    "\n",
    "# true model is linear with a = 1 and b = 1\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "n_points = 100\n",
    "\n",
    "X = np.random.rand(n_points)\n",
    "Y = a*X + b + 0.2*np.random.randn(n_points) # final term is random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.plot([0,1], [1, 2], color = \"black\", label = \"true model\")\n",
    "ax.scatter(X, Y, label = \"data\")\n",
    "ax.set(xlabel='X', ylabel='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3680c6",
   "metadata": {},
   "source": [
    "### Fit the model \n",
    "\n",
    "train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'Y': Y, 'X': X})\n",
    "train = df.sample(frac=0.8) # 80% rows for training\n",
    "test = df.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Y']\n",
    "X_train = train.drop(columns=['Y'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Y']\n",
    "X_test = test.drop(columns=['Y']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6461a-b3e3-4027-b51d-81dd6441056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be98217",
   "metadata": {},
   "source": [
    "#### Fit two polynomial regression models to the training data: one using a degree-1 polynomial (simple linear model) and the other using a degree-20 polynomial (more complex model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c887771-9df8-4aa6-9866-6151d6104ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = PolynomialRegression(1)\n",
    "model1.fit(X_train, y_train)\n",
    "model2 = PolynomialRegression(20)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826b175-b120-498c-84b6-7b8a767edf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = pd.DataFrame(data={'X': np.linspace(0.01, 1, 1000)})\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.plot(prediction_inputs, model1.predict(prediction_inputs), color = \"red\", label = \"linear\")\n",
    "ax.plot(prediction_inputs, model2.predict(prediction_inputs), color = \"green\", label = \"degree 20\")\n",
    "\n",
    "ax.scatter(X_train, y_train, marker='*', label = \"train data\")\n",
    "ax.scatter(X_test, y_test, label = \"test data\")\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ac56c",
   "metadata": {},
   "source": [
    "Model 1 fits a simple straight line to the data. It doesn't perfectly capture the training data, but it generalizes reasonably well to test data.\n",
    "\n",
    "Model 2 fits a very complex curve, so it does better on the training set than the simple linear model. But it overfits â€” it captures the noise in the training data rather than the true underlying pattern. Therefore, its predictions on the test data are worse than just predicting the mean, as we can see from the negative R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3362f-9daa-490f-9351-e98c8bdee4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model 1 with degree 1 (linear)')\n",
    "print('Train score:',model1.score(X_train, y_train))\n",
    "print('Test score:',model1.score(X_test, y_test))\n",
    "print()\n",
    "print('model 2 with degree 20')\n",
    "print('Train score:',model2.score(X_train, y_train))\n",
    "print('Test score:',model2.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55708826-c48d-4e25-b254-e5a02ace6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    lr = PolynomialRegression(degree).fit(X_train, y_train)\n",
    "    train_scores.append(lr.score(X_train, y_train))\n",
    "    test_scores.append(lr.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x=degrees, y=train_scores, label='train')\n",
    "sns.scatterplot(x=degrees, y=test_scores, label='test')\n",
    "ax.set_xlabel('Degree of polynomial regression')\n",
    "ax.set_ylabel('$R^2$ score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbd225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52adc4e",
   "metadata": {},
   "source": [
    "## (Overly) simple example from lecture slides\n",
    "\n",
    "The code below does some model fitting for the example described in class (for Rick Marks section).  This not enough data to be confident about any kind of model!  It is intentionally simple so as to illustrate potential issues of fitting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table of the data shown in the slides in class\n",
    "df=pd.DataFrame({\n",
    "    'Day': ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun','Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'],\n",
    "    'Value': [190, 189.4, 188.8, 188.2, 187.5, 187, 187.8, 187.2, 186.7, 186.2, 185.8, 185.4, 185],\n",
    "    'Value Change': [float('nan'), -0.6, -0.6, -0.6, -0.7, -0.5, 0.8, -0.6, -0.5, -0.5, -0.4, -0.4, -0.4]})\n",
    "df=df.reset_index() # add the index as a column (feature)\n",
    "df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks a lot like the slide\n",
    "sns.scatterplot(df,x='index',y='Value')\n",
    "plt.ylim(175, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85704285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at just the change in the value.  Note the potential outlier.\n",
    "sns.scatterplot(df,x='index',y='Value Change')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd55b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the change in value by day\n",
    "sns.boxplot(df,x='Day',y='Value Change')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61a3a7",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ac93e",
   "metadata": {},
   "source": [
    "### Predict Value based on index\n",
    "\n",
    "What if we try a simple linear regression model to predict Value, using the index? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ec6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df.drop(columns=['Value Change','Value','Day'])\n",
    "y1 = df['Value']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LM1 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "LM1.fit(X1, y1)\n",
    "LM1.score(X1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b222931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(df['index'], LM1.predict(X1), color = \"red\", label = \"linear regression\")\n",
    "ax.scatter(X1, y1, marker='*', label = \"train data\")\n",
    "plt.ylim(175, 200)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf757b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM1.predict(pd.DataFrame({'index': [13]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe053a",
   "metadata": {},
   "source": [
    "### Predict Value Change based on index\n",
    "\n",
    "What if instead we used simple linear regression to prediction the change in value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df.dropna().copy() # this will drop the first row because there is no value change for that sample\n",
    "df_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df_clean.drop(columns=['Value Change','Value','Day'])\n",
    "y2 = df_clean['Value Change']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LM2 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "LM2.fit(X2, y2)\n",
    "LM2.score(X2, y2) # as you see below, not a very good fit score (mostly due to value change for index 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e632b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(df_clean['index'], LM2.predict(X2), color = \"red\", label = \"linear regression\")\n",
    "plt.scatter(X2, y2, marker='*', label = \"train data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda72d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM2.predict(pd.DataFrame({'index': [13]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15dedc2",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Now let's try a decision tree on this data instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23893b0d",
   "metadata": {},
   "source": [
    "### Predict Value Change direction based on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db093aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_tree = df_clean.copy()\n",
    "# convert the value change to a categorical variable\n",
    "df_clean_tree['Value Change direction'] = 'no change'\n",
    "\n",
    "#change df_clean_tree['Value Change cat'] to increase or decrease\n",
    "df_clean_tree.loc[df_clean_tree['Value Change']<0.0, 'Value Change direction'] = 'decrease'  \n",
    "df_clean_tree.loc[df_clean_tree['Value Change']>0.0, 'Value Change direction'] = 'increase'\n",
    "\n",
    "df_clean_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25042d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt1 = df_clean_tree.drop(columns=['Value Change','Value','Day','Value Change direction'])\n",
    "yt1 = df_clean_tree['Value Change direction']\n",
    "from sklearn import tree\n",
    "T1 = tree.DecisionTreeClassifier(max_depth=2)\n",
    "T1.fit(Xt1, yt1)\n",
    "fig, ax = plt.subplots(1, figsize = (8, 8))\n",
    "p = tree.plot_tree(T1, \n",
    "                   filled=True, \n",
    "                   feature_names=Xt1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T1.predict(pd.DataFrame({'index': [13]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed12a4",
   "metadata": {},
   "source": [
    "### Predict Value Change direction based on Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cff8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the Day to a numeric variable so we can use it in the decision tree\n",
    "# this is not \n",
    "df_clean_tree['Day numeric'] = df_clean_tree['Day'].map({'Mon': 0, 'Tue': 1, 'Wed': 2, 'Thu': 3, 'Fri': 4, 'Sat': 5, 'Sun': 6})\n",
    "df_clean_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt2 = df_clean_tree.drop(columns=['Value Change','Value','Day','Value Change direction','index'])\n",
    "yt2 = df_clean_tree['Value Change direction']\n",
    "from sklearn import tree\n",
    "T2 = tree.DecisionTreeClassifier(max_depth=2)\n",
    "T2.fit(Xt2, yt2)\n",
    "fig, ax = plt.subplots(1, figsize = (8, 8))\n",
    "p = tree.plot_tree(T2, \n",
    "                   filled=True, \n",
    "                   feature_names=Xt2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "T2.predict(pd.DataFrame({'Day numeric': [6]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993b3df",
   "metadata": {},
   "source": [
    "# Predicting the temperament of ROUSes using k-NN classification\n",
    "Using other data we have in the table, we want to predict the temperament of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1cd0e",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "First, let's look at a scatterplot with the temperament represented as color and symbols to get a general idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Age',y='Length', hue='Temperament', style='Temperament')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b81bf7",
   "metadata": {},
   "source": [
    "As you can see, there are some clusters of the same temperament, which means the samples have the same temperament as their neighbors, so k-NN should work well for those.  But there are also definitely some samples are more \"alone\" so k-NN won't be as good for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f7aec",
   "metadata": {},
   "source": [
    "### Normalize columns\n",
    "The next cell normalizes the columns so the neighbor distance calculations will be scaled equivalently.  You can try skipping this cell to see the performance without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try skipping this cell and see results without scaling\n",
    "rouses['Age'] = (rouses['Age']-rouses['Age'].min())/( rouses['Age'].max()-rouses['Age'].min()) # normalize 'Age' columns\n",
    "rouses['Length'] = (rouses['Length']-rouses['Length'].min())/( rouses['Length'].max()-rouses['Length'].min()) # normalize 'Length' columns\n",
    "rouses['Weight'] = (rouses['Weight']-rouses['Weight'].min())/( rouses['Weight'].max()-rouses['Weight'].min()) # normalize 'Weight' columns\n",
    "\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df3e1f",
   "metadata": {},
   "source": [
    "Okay, as usual let's follow the train and test process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d675c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=1234) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Temperament']\n",
    "X_train = train.drop(columns=['Temperament'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Temperament']\n",
    "X_test = test.drop(columns=['Temperament']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b21984",
   "metadata": {},
   "source": [
    "Notice that is a very small number of train and test samples, so our results are going to be highly dependent on how the data is split.  Let's try k-NN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a11200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)\n",
    "print('Prediction:',knn.predict(X_test))\n",
    "print('Actual:',list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c53bde",
   "metadata": {},
   "source": [
    "Well, predicting 4 out of 6 isn't great, but it is actually better than expected considering the Train score.  Predicting temperament is a pretty tough challenge!  Try different values for k (`n_neighbors`) to see what changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce30589",
   "metadata": {},
   "source": [
    "# Predicting the weight of ROUSes using k-NN\n",
    "Using other data we have in the table, we want to predict the weight of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3fe7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Length',y='Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b800af",
   "metadata": {},
   "source": [
    "In our previous linear regression work, we were able to use `Age` to predict `Weight` quite well because the correlation was close to linear.  Let's try using `Length` instead, and see how well we can predict despite the relationship being less linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = rouses.drop(columns=['Temperament','Age']) # drop the columns 'Temperament' and 'Age'\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51f4c0",
   "metadata": {},
   "source": [
    "Train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=4321) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a26a2",
   "metadata": {},
   "source": [
    "The next thing to do is to separate out the target data `Weight` from the predictor data (everything else; in this case just `Length` is left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Weight']\n",
    "X_train = train.drop(columns=['Weight'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Weight']\n",
    "X_test = test.drop(columns=['Weight']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8f9bc",
   "metadata": {},
   "source": [
    "Okay, let's try using weighted K-NN for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207400e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364953c",
   "metadata": {},
   "source": [
    "For regression a \"score\" (the R2 value) near 1 is what we are hoping for, and 0 is the worst result.  So our model is doing a very good job at predicting the data!\n",
    "\n",
    "To visualize, we can plug in the test values in and have their outputs predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(train,x='Length', y='Weight', label = \"training data\")\n",
    "ax.scatter(test['Length'], predictions, label = \"test\")\n",
    "ax.set(xlabel='Length', ylabel='Weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ed579",
   "metadata": {},
   "source": [
    "Indeed, it looks like these predictions follow the trend of the data! You can try different values for k to see how the results change. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
